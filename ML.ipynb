{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbd903ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import requests\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import ssl\n",
    "import joblib\n",
    "\n",
    "import firebase_admin\n",
    "from firebase_admin import credentials, firestore, db\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 서비스 계정 키 파일 경로\n",
    "cred = credentials.Certificate('./fishing-pishing-firebase-adminsdk-kdegk-8d4c2fda5d.json')\n",
    "\n",
    "if not firebase_admin._apps:\n",
    "    firebase_admin.initialize_app(cred, {\n",
    "        'databaseURL': 'https://fishing-pishing-default-rtdb.firebaseio.com'\n",
    "    })\n",
    "\n",
    "######### 키워드 추출 ##############\n",
    "# 파이어베이스에서 'keyword' 노드의 데이터를 불러옴\n",
    "keyword_ref = db.reference('keyword')\n",
    "keywordsData = keyword_ref.get()\n",
    "\n",
    "# 데이터를 DataFrame으로 변환\n",
    "keywords_list = []\n",
    "for key, value in keywordsData.items():\n",
    "    keywords_list.append(value)\n",
    "\n",
    "keywordsDf = pd.DataFrame(keywords_list)\n",
    "\n",
    "# 'Keywords'와 'return' 칼럼 추출\n",
    "keywords = keywordsDf['Keywords']\n",
    "category = keywordsDf['return']\n",
    "\n",
    "category_num = []\n",
    "\n",
    "def extract_keywords(text_tokens, keywords):\n",
    "    i = 0\n",
    "    extracted_words = []   # 문자 텍스트 내에 포함되어 있는 키워드를 포함하고 있는 단어 리스트\n",
    "    key_tokens = []        # 문자 텍스트 내에 포함되어 있는 키워드 리스트\n",
    "    key_tokens_index = []  # 문자 텍스트 내에 포함되어 있는 키워드가 keywords DataFrame에서 몇번째 인덱스인지\n",
    "    for word in text_tokens:\n",
    "        for i in range(len(keywords)):\n",
    "            if keywords[i] in word:\n",
    "                extracted_words.append(word)\n",
    "                key_tokens.append(keywords[i])\n",
    "                key_tokens_index.append(i)\n",
    "    \n",
    "    return key_tokens, key_tokens_index\n",
    "\n",
    "\n",
    "######### 키워드의 카테고리 추출 #############\n",
    "def find_category(category, key_tokens_index, category_num):\n",
    "    for index in key_tokens_index:\n",
    "        category_num.append(category[index])\n",
    "    category_num = set(category_num)\n",
    "    category_num = list(category_num)\n",
    "    return category_num\n",
    "\n",
    "\n",
    "########### url 추출 #############\n",
    "def extract_urls_1(text_tokens):\n",
    "    \n",
    "    urls = []\n",
    "    url_pattern = re.compile(r'\\b(?:https?://|www\\.)\\S+\\b')\n",
    "\n",
    "    # 정규 표현식과 매치되는 모든 URL 추출\n",
    "    for token in text_tokens:\n",
    "        result = re.match(url_pattern, token)\n",
    "        if result != None:\n",
    "            urls.append(token)\n",
    "    return urls\n",
    "\n",
    "def extract_urls_2(token):\n",
    "    try:\n",
    "        response = requests.get('https://' + token, timeout=3, verify=False)\n",
    "        if response.status_code == 200:\n",
    "            token = \"https://\"+token\n",
    "            return token\n",
    "    except Exception as e:\n",
    "        logging.error(f\"HTTPS 연결 오류 - {token}: {e}\")\n",
    "        \n",
    "    \n",
    "    try:\n",
    "        response = requests.get('http://' + token, timeout=3)\n",
    "        if response.status_code == 200:\n",
    "            token = \"http://\"+token\n",
    "            return token\n",
    "    except Exception as e:\n",
    "        logging.error(f\"HTTP 연결 오류 - {token}: {e}\")\n",
    "        \n",
    "\n",
    "\n",
    "############ 최종 판별 함수 ##############\n",
    "def url_keywords_extraction(text, keywordDf):\n",
    "\n",
    "    \n",
    "    text_tokens = list(text.split())\n",
    "    \n",
    "    # URL 추출\n",
    "    urls1 = extract_urls_1(text_tokens)\n",
    "    urls2 = []\n",
    "    for token in text_tokens:\n",
    "        urls2.append(extract_urls_2(token))\n",
    "    \n",
    "    # 추출된 url 리스트\n",
    "    urls = urls1+urls2\n",
    "    urls = [url for url in urls if url is not None]\n",
    "    \n",
    "    if len(urls) <= 0:\n",
    "        return \"url x 스미싱 위험도 낮음\"\n",
    "    \n",
    "    # 키워드 추출 및 키워드의 카테고리 찾기\n",
    "    keywords, keywords_index = extract_keywords(text_tokens, keywordDf)\n",
    "    \n",
    "    # 키워드가 없을 경우 - ML 돌리기\n",
    "    if len(keywords) <= 0:\n",
    "        for url in urls:\n",
    "            result = search_in_model(url)\n",
    "            if result == 1:\n",
    "                return \"ml 스미싱 위험도 높음\"\n",
    "        return \"ml 스미싱 위험도 낮음\"\n",
    "\n",
    "\n",
    "    url_dataset_num = find_category(category, keywords_index, category_num)\n",
    "\n",
    "    for url in urls:\n",
    "        result = search_in_firebase(url_dataset_num, url)\n",
    "        return \"db 스미싱 위험도 낮음\"\n",
    "            \n",
    "    else:\n",
    "        for url in urls:\n",
    "            result = search_in_model(url)\n",
    "            if result == 1:\n",
    "                return \"db - ml스미싱 위험도 높음\"\n",
    "        return \"db-ml 스미싱 위험도 낮음\"\n",
    "\n",
    "############## 리턴 값에 따른 스미싱 판단 ##############\n",
    "# 1. url 없음 -> 스미싱 아님 출력\n",
    "# 2. 키워드 없음 -> ML 판단 -> 결과 출력\n",
    "# 3. 키워드 있음 -> 키워드 카테고리 whitelist 대조 -> (whitelist에 해당 url 없으면) ML 판단 -> 결과 출력\n",
    "\n",
    "############## whitelist 대조 ##############\n",
    "# URL whitelist 검색 함수 / 입력 (카테고리 리스트, 검색할 url) / 반환 (카테고리, 해당 Url의 Name값)\n",
    "def search_in_firebase(node_names, search_url):\n",
    "    \n",
    "    for node_name in node_names:\n",
    "        # 해당 노드의 참조를 가져옴\n",
    "        whitelist_ref = db.reference(f'whitelist/{node_name}')\n",
    "        \n",
    "        # 참조에서 전체 데이터를 가져옴\n",
    "        whitelist_data = whitelist_ref.get()\n",
    "\n",
    "        # 데이터가 없으면 다음 노드로 계속\n",
    "        if not whitelist_data:\n",
    "            continue\n",
    "        \n",
    "        # 데이터를 순회하며 검색 URL이 있는지 확인\n",
    "        for key, value in whitelist_data.items():\n",
    "            if 'Url' in value and value['Url'] == search_url:\n",
    "                # 'Name' 값과 함께 반환, 'Name' 필드가 없을 경우 None 반환\n",
    "                return True\n",
    "\n",
    "    return False  # URL이 데이터베이스에 없음\n",
    "\n",
    "############ feature 추출 함수 ###############\n",
    "# 피처 추출\n",
    "import ipaddress\n",
    "import re\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import socket\n",
    "import requests\n",
    "from googlesearch import search\n",
    "import whois\n",
    "from datetime import date, datetime\n",
    "import time\n",
    "from dateutil.parser import parse as date_parse\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "class FeatureExtraction:\n",
    "    features = []\n",
    "    def __init__(self,url):\n",
    "        self.features = []\n",
    "        self.url = url\n",
    "        self.domain = \"\"\n",
    "        self.whois_response = \"\"\n",
    "        self.urlparse = \"\"\n",
    "        self.response = \"\"\n",
    "        self.soup = \"\"\n",
    "\n",
    "        try:\n",
    "            self.response = requests.get(url)\n",
    "            self.soup = BeautifulSoup(self.response.text, 'html.parser')\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating BeautifulSoup object: {e}\")\n",
    "            self.soup = None\n",
    "\n",
    "        try:\n",
    "            self.urlparse = urlparse(url)\n",
    "            self.domain = self.urlparse.netloc\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            self.whois_response = whois.whois(self.domain)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        self.features.append(self.UsingIp())\n",
    "        self.features.append(self.longUrl())\n",
    "        self.features.append(self.shortUrl())\n",
    "        self.features.append(self.symbol())\n",
    "        self.features.append(self.redirecting())\n",
    "        self.features.append(self.prefixSuffix())\n",
    "        self.features.append(self.SubDomains())\n",
    "        self.features.append(self.Hppts())\n",
    "        self.features.append(self.DomainRegLen())\n",
    "        self.features.append(self.Favicon())\n",
    "\n",
    "        self.features.append(self.NonStdPort())\n",
    "        self.features.append(self.HTTPSDomainURL())\n",
    "        self.features.append(self.RequestURL())\n",
    "        self.features.append(self.AnchorURL())\n",
    "        self.features.append(self.LinksInScriptTags())\n",
    "        self.features.append(self.ServerFormHandler())\n",
    "        self.features.append(self.InfoEmail())\n",
    "        self.features.append(self.WebsiteForwarding())\n",
    "        self.features.append(self.StatusBarCust())\n",
    "\n",
    "        self.features.append(self.UsingPopupWindow())\n",
    "        self.features.append(self.IframeRedirection())\n",
    "        self.features.append(self.AgeofDomain())\n",
    "        self.features.append(self.DNSRecording())\n",
    "        self.features.append(self.LinksPointingToPage())\n",
    "        self.features.append(self.StatsReport())\n",
    "\n",
    "\n",
    "    # 1.UsingIp\n",
    "    def UsingIp(self):\n",
    "        try:\n",
    "            # 도메인에서 포트 번호를 제거합니다.\n",
    "            domain_without_port = self.domain.split(':')[0]\n",
    "            ipaddress.ip_address(domain_without_port)\n",
    "            return -1\n",
    "        except:\n",
    "            return 1\n",
    "\n",
    "    # 2.longUrl\n",
    "    def longUrl(self):\n",
    "        if len(self.url) < 54:\n",
    "            return 1\n",
    "        if len(self.url) >= 54 and len(self.url) <= 75:\n",
    "            return 0\n",
    "        return -1\n",
    "\n",
    "    # 3.shortUrl\n",
    "    def shortUrl(self):\n",
    "        match = re.search('bit\\.ly|goo\\.gl|shorte\\.st|go2l\\.ink|x\\.co|ow\\.ly|t\\.co|tinyurl|tr\\.im|is\\.gd|cli\\.gs|'\n",
    "                    'yfrog\\.com|migre\\.me|ff\\.im|tiny\\.cc|url4\\.eu|twit\\.ac|su\\.pr|twurl\\.nl|snipurl\\.com|'\n",
    "                    'short\\.to|BudURL\\.com|ping\\.fm|post\\.ly|Just\\.as|bkite\\.com|snipr\\.com|fic\\.kr|loopt\\.us|'\n",
    "                    'doiop\\.com|short\\.ie|kl\\.am|wp\\.me|rubyurl\\.com|om\\.ly|to\\.ly|bit\\.do|t\\.co|lnkd\\.in|'\n",
    "                    'db\\.tt|qr\\.ae|adf\\.ly|goo\\.gl|bitly\\.com|cur\\.lv|tinyurl\\.com|ow\\.ly|bit\\.ly|ity\\.im|'\n",
    "                    'q\\.gs|is\\.gd|po\\.st|bc\\.vc|twitthis\\.com|u\\.to|j\\.mp|buzurl\\.com|cutt\\.us|u\\.bb|yourls\\.org|'\n",
    "                    'x\\.co|prettylinkpro\\.com|scrnch\\.me|filoops\\.info|vzturl\\.com|qr\\.net|1url\\.com|tweez\\.me|v\\.gd|tr\\.im|link\\.zip\\.net', self.url)\n",
    "        if match:\n",
    "            return -1\n",
    "        return 1\n",
    "\n",
    "    # 4.Symbol@\n",
    "    def symbol(self):\n",
    "        if re.findall(\"@\",self.url):\n",
    "            return -1\n",
    "        return 1\n",
    "    \n",
    "    # 5.Redirecting//\n",
    "    def redirecting(self):\n",
    "        if self.url.rfind('//')>6:\n",
    "            return -1\n",
    "        return 1\n",
    "    \n",
    "    # 6.prefixSuffix\n",
    "    def prefixSuffix(self):\n",
    "        try:\n",
    "            match = re.findall('\\-', self.domain)\n",
    "            if match:\n",
    "                return -1\n",
    "            return 1\n",
    "        except:\n",
    "            return -1\n",
    "    \n",
    "    # 7.SubDomains\n",
    "    def SubDomains(self):\n",
    "        dot_count = len(re.findall(\"\\.\", self.url))\n",
    "        if dot_count == 1:\n",
    "            return 1\n",
    "        elif dot_count == 2:\n",
    "            return 0\n",
    "        return -1\n",
    "\n",
    "    # 8.HTTPS\n",
    "    def Hppts(self):\n",
    "        try:\n",
    "            https = self.urlparse.scheme\n",
    "            if 'https' in https:\n",
    "                return 1\n",
    "            return -1\n",
    "        except:\n",
    "            return 1\n",
    "\n",
    "    # 9.DomainRegLen\n",
    "    def DomainRegLen(self):\n",
    "        try:\n",
    "            expiration_date = self.whois_response.expiration_date\n",
    "            creation_date = self.whois_response.creation_date\n",
    "            try:\n",
    "                if(len(expiration_date)):\n",
    "                    expiration_date = expiration_date[0]\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                if(len(creation_date)):\n",
    "                    creation_date = creation_date[0]\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            age = (expiration_date.year-creation_date.year)*12+ (expiration_date.month-creation_date.month)\n",
    "            if age >=12:\n",
    "                return 1\n",
    "            return -1\n",
    "        except:\n",
    "            return -1\n",
    "\n",
    "    # 10. Favicon\n",
    "    def Favicon(self):\n",
    "        if self.soup == None:\n",
    "            return 2\n",
    "        try:\n",
    "            for head in self.soup.find_all('head'):\n",
    "                for link in head.find_all('link', href=True):\n",
    "                    favicon_url = link['href']\n",
    "                    # 상대 경로를 절대 경로로 변환\n",
    "                    if favicon_url.startswith('/'):\n",
    "                        favicon_url = urllib.parse.urljoin(self.url, favicon_url)\n",
    "                    dots = [x.start(0) for x in re.finditer('\\.', favicon_url)]\n",
    "                    if self.url in favicon_url or len(dots) == 1 or self.domain in favicon_url:\n",
    "                        return 1\n",
    "            return -1\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            return -1\n",
    "\n",
    "\n",
    "    # 11. NonStdPort\n",
    "    def NonStdPort(self):\n",
    "        try:\n",
    "            port = self.domain.split(\":\")\n",
    "            if len(port)>1:\n",
    "                return -1\n",
    "            return 1\n",
    "        except:\n",
    "            return -1\n",
    "\n",
    "    # 12. HTTPSDomainURL\n",
    "    def HTTPSDomainURL(self):\n",
    "        try:\n",
    "            if 'https' in self.url:\n",
    "                return -1\n",
    "            return 1\n",
    "        except:\n",
    "            return -1\n",
    "    \n",
    "    # 13. RequestURL\n",
    "    def RequestURL(self):\n",
    "        if self.soup == None:\n",
    "            return 2\n",
    "        try:\n",
    "            for img in self.soup.find_all('img', src=True):\n",
    "                dots = [x.start(0) for x in re.finditer('\\.', img['src'])]\n",
    "                if self.url in img['src'] or self.domain in img['src'] or len(dots) == 1:\n",
    "                    success = success + 1\n",
    "                i = i+1\n",
    "\n",
    "            for audio in self.soup.find_all('audio', src=True):\n",
    "                dots = [x.start(0) for x in re.finditer('\\.', audio['src'])]\n",
    "                if self.url in audio['src'] or self.domain in audio['src'] or len(dots) == 1:\n",
    "                    success = success + 1\n",
    "                i = i+1\n",
    "\n",
    "            for embed in self.soup.find_all('embed', src=True):\n",
    "                dots = [x.start(0) for x in re.finditer('\\.', embed['src'])]\n",
    "                if self.url in embed['src'] or self.domain in embed['src'] or len(dots) == 1:\n",
    "                    success = success + 1\n",
    "                i = i+1\n",
    "\n",
    "            for iframe in self.soup.find_all('iframe', src=True):\n",
    "                dots = [x.start(0) for x in re.finditer('\\.', iframe['src'])]\n",
    "                if self.url in iframe['src'] or self.domain in iframe['src'] or len(dots) == 1:\n",
    "                    success = success + 1\n",
    "                i = i+1\n",
    "\n",
    "            try:\n",
    "                percentage = success/float(i) * 100\n",
    "                if percentage < 22.0:\n",
    "                    return 1\n",
    "                elif((percentage >= 22.0) and (percentage < 61.0)):\n",
    "                    return 0\n",
    "                else:\n",
    "                    return -1\n",
    "            except:\n",
    "                return 0\n",
    "        except:\n",
    "            return -1\n",
    "    \n",
    "    # 14. AnchorURL\n",
    "    def AnchorURL(self):\n",
    "        if self.soup == None:\n",
    "            return 2\n",
    "        try:\n",
    "            i,unsafe = 0,0\n",
    "            for a in self.soup.find_all('a', href=True):\n",
    "                if \"#\" in a['href'] or \"javascript\" in a['href'].lower() or \"mailto\" in a['href'].lower() or not (url in a['href'] or self.domain in a['href']):\n",
    "                    unsafe = unsafe + 1\n",
    "                i = i + 1\n",
    "\n",
    "            try:\n",
    "                percentage = unsafe / float(i) * 100\n",
    "                if percentage < 31.0:\n",
    "                    return 1\n",
    "                elif ((percentage >= 31.0) and (percentage < 67.0)):\n",
    "                    return 0\n",
    "                else:\n",
    "                    return -1\n",
    "            except:\n",
    "                return -1\n",
    "\n",
    "        except:\n",
    "            return -1\n",
    "\n",
    "    # 15. LinksInScriptTags\n",
    "    def LinksInScriptTags(self):\n",
    "        if self.soup == None:\n",
    "            return 2\n",
    "        try:\n",
    "            i,success = 0,0\n",
    "        \n",
    "            for link in self.soup.find_all('link', href=True):\n",
    "                dots = [x.start(0) for x in re.finditer('\\.', link['href'])]\n",
    "                if self.url in link['href'] or self.domain in link['href'] or len(dots) == 1:\n",
    "                    success = success + 1\n",
    "                i = i+1\n",
    "\n",
    "            for script in self.soup.find_all('script', src=True):\n",
    "                dots = [x.start(0) for x in re.finditer('\\.', script['src'])]\n",
    "                if self.url in script['src'] or self.domain in script['src'] or len(dots) == 1:\n",
    "                    success = success + 1\n",
    "                i = i+1\n",
    "\n",
    "            try:\n",
    "                percentage = success / float(i) * 100\n",
    "                if percentage < 17.0:\n",
    "                    return 1\n",
    "                elif((percentage >= 17.0) and (percentage < 81.0)):\n",
    "                    return 0\n",
    "                else:\n",
    "                    return -1\n",
    "            except:\n",
    "                return 0\n",
    "        except:\n",
    "            return -1\n",
    "\n",
    "    # 16. ServerFormHandler\n",
    "    def ServerFormHandler(self):\n",
    "        if self.soup == None:\n",
    "            return 2\n",
    "        try:\n",
    "            if len(self.soup.find_all('form', action=True))==0:\n",
    "                return 1\n",
    "            else :\n",
    "                for form in self.soup.find_all('form', action=True):\n",
    "                    if form['action'] == \"\" or form['action'] == \"about:blank\":\n",
    "                        return -1\n",
    "                    elif self.url not in form['action'] and self.domain not in form['action']:\n",
    "                        return 0\n",
    "                    else:\n",
    "                        return 1\n",
    "        except:\n",
    "            return -1\n",
    "\n",
    "    # 17. InfoEmail\n",
    "    def InfoEmail(self):\n",
    "        if self.soup == None:\n",
    "            return 2\n",
    "        try:\n",
    "            if re.findall(r\"mailto:\", self.soup):\n",
    "                return -1\n",
    "            else:\n",
    "                return 1\n",
    "        except:\n",
    "            return -1\n",
    "\n",
    "    # 18. WebsiteForwarding\n",
    "    def WebsiteForwarding(self):\n",
    "        try:\n",
    "            if len(self.response.history) <= 1:\n",
    "                return 1\n",
    "            elif len(self.response.history) <= 4:\n",
    "                return 0\n",
    "            else:\n",
    "                return -1\n",
    "        except:\n",
    "             return -1\n",
    "\n",
    "    # 19. StatusBarCust\n",
    "    def StatusBarCust(self):\n",
    "        try:\n",
    "            if re.findall(\"<script>.+onmouseover.+</script>\", self.response.text):\n",
    "                return 1\n",
    "            else:\n",
    "                return -1\n",
    "        except:\n",
    "             return -1\n",
    "\n",
    "    # 20. UsingPopupWindow\n",
    "    def UsingPopupWindow(self):\n",
    "        try:\n",
    "            if re.findall(r\"alert\\(\", self.response.text):\n",
    "                return 1\n",
    "            else:\n",
    "                return -1\n",
    "        except:\n",
    "             return -1\n",
    "\n",
    "    # 21. IframeRedirection\n",
    "    def IframeRedirection(self):\n",
    "        try:\n",
    "            if re.findall(r\"[<iframe>|<frameBorder>]\", self.response.text):\n",
    "                return 1\n",
    "            else:\n",
    "                return -1\n",
    "        except:\n",
    "             return -1\n",
    "\n",
    "    # 22. AgeofDomain\n",
    "    def AgeofDomain(self):\n",
    "        try:\n",
    "            creation_date = self.whois_response.creation_date\n",
    "            try:\n",
    "                if(len(creation_date)):\n",
    "                    creation_date = creation_date[0]\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            today  = date.today()\n",
    "            age = (today.year-creation_date.year)*12+(today.month-creation_date.month)\n",
    "            if age >=6:\n",
    "                return 1\n",
    "            return -1\n",
    "        except:\n",
    "            return -1\n",
    "\n",
    "    # 23. DNSRecording    \n",
    "    def DNSRecording(self):\n",
    "        try:\n",
    "            creation_date = self.whois_response.creation_date\n",
    "            try:\n",
    "                if(len(creation_date)):\n",
    "                    creation_date = creation_date[0]\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            today  = date.today()\n",
    "            age = (today.year-creation_date.year)*12+(today.month-creation_date.month)\n",
    "            if age >=6:\n",
    "                return 1\n",
    "            return -1\n",
    "        except:\n",
    "            return -1\n",
    "\n",
    "    # 24. LinksPointingToPage\n",
    "    def LinksPointingToPage(self):\n",
    "        try:\n",
    "            number_of_links = len(re.findall(r\"<a href=\", self.response.text))\n",
    "            if number_of_links == 0:\n",
    "                return 1\n",
    "            elif number_of_links <= 2:\n",
    "                return 0\n",
    "            else:\n",
    "                return -1\n",
    "        except:\n",
    "            return -1\n",
    "\n",
    "    # 25. StatsReport\n",
    "    def StatsReport(self):\n",
    "        try:\n",
    "            url_match = re.search(\n",
    "        'at\\.ua|usa\\.cc|baltazarpresentes\\.com\\.br|pe\\.hu|esy\\.es|hol\\.es|sweddy\\.com|myjino\\.ru|96\\.lt|ow\\.ly', self.url)\n",
    "            try:\n",
    "                ip_address = socket.gethostbyname(self.domain)\n",
    "            except Exception as e:\n",
    "                print(f\"Error in DNS lookup: {e}\")\n",
    "\n",
    "            ip_match = re.search('146\\.112\\.61\\.108|213\\.174\\.157\\.151|121\\.50\\.168\\.88|192\\.185\\.217\\.116|78\\.46\\.211\\.158|181\\.174\\.165\\.13|46\\.242\\.145\\.103|121\\.50\\.168\\.40|83\\.125\\.22\\.219|46\\.242\\.145\\.98|'\n",
    "                                '107\\.151\\.148\\.44|107\\.151\\.148\\.107|64\\.70\\.19\\.203|199\\.184\\.144\\.27|107\\.151\\.148\\.108|107\\.151\\.148\\.109|119\\.28\\.52\\.61|54\\.83\\.43\\.69|52\\.69\\.166\\.231|216\\.58\\.192\\.225|'\n",
    "                                '118\\.184\\.25\\.86|67\\.208\\.74\\.71|23\\.253\\.126\\.58|104\\.239\\.157\\.210|175\\.126\\.123\\.219|141\\.8\\.224\\.221|10\\.10\\.10\\.10|43\\.229\\.108\\.32|103\\.232\\.215\\.140|69\\.172\\.201\\.153|'\n",
    "                                '216\\.218\\.185\\.162|54\\.225\\.104\\.146|103\\.243\\.24\\.98|199\\.59\\.243\\.120|31\\.170\\.160\\.61|213\\.19\\.128\\.77|62\\.113\\.226\\.131|208\\.100\\.26\\.234|195\\.16\\.127\\.102|195\\.16\\.127\\.157|'\n",
    "                                '34\\.196\\.13\\.28|103\\.224\\.212\\.222|172\\.217\\.4\\.225|54\\.72\\.9\\.51|192\\.64\\.147\\.141|198\\.200\\.56\\.183|23\\.253\\.164\\.103|52\\.48\\.191\\.26|52\\.214\\.197\\.72|87\\.98\\.255\\.18|209\\.99\\.17\\.27|'\n",
    "                                '216\\.38\\.62\\.18|104\\.130\\.124\\.96|47\\.89\\.58\\.141|78\\.46\\.211\\.158|54\\.86\\.225\\.156|54\\.82\\.156\\.19|37\\.157\\.192\\.102|204\\.11\\.56\\.48|110\\.34\\.231\\.42', ip_address)\n",
    "            if url_match:\n",
    "                return -1\n",
    "            elif ip_match:\n",
    "                return -1\n",
    "            return 1\n",
    "        except:\n",
    "            return 1\n",
    "    \n",
    "    def getFeaturesList(self):\n",
    "        return self.features\n",
    "\n",
    "############# ML Model에 넣기 ##############\n",
    "def search_in_model(url):\n",
    "    model = joblib.load('./url_classification_model_DecisionTree.pkl')\n",
    "    fe = FeatureExtraction(url)\n",
    "    url_info = fe.getFeaturesList()\n",
    "    url_features = np.array(url_info)\n",
    "    result = model.predict(url_features.reshape(1, -1))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b39943d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url이 없는 메세지:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:HTTPS 연결 오류 - 카드배송업체: HTTPSConnectionPool(host='xn--hy1b02loxefzeuxkxuc', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001AE9C3BF190>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "ERROR:root:HTTP 연결 오류 - 카드배송업체: HTTPConnectionPool(host='xn--hy1b02loxefzeuxkxuc', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001AE9C3BE4D0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "ERROR:root:HTTPS 연결 오류 - 마그넷서비스: HTTPSConnectionPool(host='xn--2i0bp5d68jjxe85dixc', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001AE9C3BE950>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "ERROR:root:HTTP 연결 오류 - 마그넷서비스: HTTPConnectionPool(host='xn--2i0bp5d68jjxe85dixc', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001AE9C3BE2D0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "ERROR:root:HTTPS 연결 오류 - [롯데카드]: Failed to parse: https://[롯데카드]\n",
      "ERROR:root:HTTP 연결 오류 - [롯데카드]: Failed to parse: http://[롯데카드]\n",
      "ERROR:root:HTTPS 연결 오류 - 정*은님이: Failed to parse: https://정*은님이\n",
      "ERROR:root:HTTP 연결 오류 - 정*은님이: Failed to parse: http://정*은님이\n",
      "ERROR:root:HTTPS 연결 오류 - 신청하신: HTTPSConnectionPool(host='xn--zv4ba046ev3i', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001AE9C171F90>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "ERROR:root:HTTP 연결 오류 - 신청하신: HTTPConnectionPool(host='xn--zv4ba046ev3i', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001AE9C3BF410>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "ERROR:root:HTTPS 연결 오류 - 카드가: HTTPSConnectionPool(host='xn--o39a01oulw', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001AE9C3BF190>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "ERROR:root:HTTP 연결 오류 - 카드가: HTTPConnectionPool(host='xn--o39a01oulw', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001AE9C3BD810>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "ERROR:root:HTTPS 연결 오류 - 오늘: HTTPSConnectionPool(host='xn--wh1br48a', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001AE9C3BCF50>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "ERROR:root:HTTP 연결 오류 - 오늘: HTTPConnectionPool(host='xn--wh1br48a', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001AE9C1702D0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "ERROR:root:HTTPS 연결 오류 - 11:00~13:00: Failed to parse: https://11:00~13:00\n",
      "ERROR:root:HTTP 연결 오류 - 11:00~13:00: Failed to parse: http://11:00~13:00\n",
      "ERROR:root:HTTPS 연결 오류 - 배송될: HTTPSConnectionPool(host='xn--3s1bx6mote', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001AE9C28C990>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "ERROR:root:HTTP 연결 오류 - 배송될: HTTPConnectionPool(host='xn--3s1bx6mote', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001AE9C28CF50>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "ERROR:root:HTTPS 연결 오류 - 예정: HTTPSConnectionPool(host='xn--2j5bo1b', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001AE9C28CF50>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "ERROR:root:HTTP 연결 오류 - 예정: HTTPConnectionPool(host='xn--2j5bo1b', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001AE9C28DD90>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "ERROR:root:HTTPS 연결 오류 - 입니다.: HTTPSConnectionPool(host='xn--9i1b3bt39f.', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001AE9C28D990>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "ERROR:root:HTTP 연결 오류 - 입니다.: HTTPConnectionPool(host='xn--9i1b3bt39f.', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001AE9C28DE90>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "ERROR:root:HTTPS 연결 오류 - ■: Failed to parse: https://■\n",
      "ERROR:root:HTTP 연결 오류 - ■: Failed to parse: http://■\n",
      "ERROR:root:HTTPS 연결 오류 - 배송지: HTTPSConnectionPool(host='xn--2h3bo3hh5h', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001AE9C28DA90>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "ERROR:root:HTTP 연결 오류 - 배송지: HTTPConnectionPool(host='xn--2h3bo3hh5h', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001AE9C28E950>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "ERROR:root:HTTPS 연결 오류 - 주소:: HTTPSConnectionPool(host='xn--9l4b19k', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001AE9C28E090>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "ERROR:root:HTTP 연결 오류 - 주소:: HTTPConnectionPool(host='xn--9l4b19k', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001AE9C28E6D0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "ERROR:root:HTTPS 연결 오류 - 서울: HTTPSConnectionPool(host='xn--2i4bq6h', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001AE9C28E5D0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "ERROR:root:HTTP 연결 오류 - 서울: HTTPConnectionPool(host='xn--2i4bq6h', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001AE9C28EC10>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "ERROR:root:HTTPS 연결 오류 - 성북구: HTTPSConnectionPool(host='xn--2e0bz81a5wc', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001AE9C28E910>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "ERROR:root:HTTP 연결 오류 - 성북구: HTTPConnectionPool(host='xn--2e0bz81a5wc', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001AE9C28F590>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "ERROR:root:HTTPS 연결 오류 - 장위로51길: HTTPSConnectionPool(host='xn--51-752i184a8nsu1a', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001AE9C28F850>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "ERROR:root:HTTP 연결 오류 - 장위로51길: HTTPConnectionPool(host='xn--51-752i184a8nsu1a', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001AE9C28E9D0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "ERROR:root:HTTPS 연결 오류 - 15: HTTPSConnectionPool(host='15', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001AE9C28E6D0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:HTTP 연결 오류 - 15: HTTPConnectionPool(host='15', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001AE9C28F2D0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "ERROR:root:HTTPS 연결 오류 - ■: Failed to parse: https://■\n",
      "ERROR:root:HTTP 연결 오류 - ■: Failed to parse: http://■\n",
      "ERROR:root:HTTPS 연결 오류 - 배송업체:: HTTPSConnectionPool(host='xn--2h3bo3h41d40i', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001AE9C28F550>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "ERROR:root:HTTP 연결 오류 - 배송업체:: HTTPConnectionPool(host='xn--2h3bo3h41d40i', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001AE9C3BD550>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "ERROR:root:HTTPS 연결 오류 - 마그넷서비스: HTTPSConnectionPool(host='xn--2i0bp5d68jjxe85dixc', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001AE9C28EB10>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "ERROR:root:HTTP 연결 오류 - 마그넷서비스: HTTPConnectionPool(host='xn--2i0bp5d68jjxe85dixc', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001AE9C3BD550>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "ERROR:root:HTTPS 연결 오류 - ■: Failed to parse: https://■\n",
      "ERROR:root:HTTP 연결 오류 - ■: Failed to parse: http://■\n",
      "ERROR:root:HTTPS 연결 오류 - 배송원:: HTTPSConnectionPool(host='xn--2h3bo3h03e', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001AE9C285050>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "ERROR:root:HTTP 연결 오류 - 배송원:: HTTPConnectionPool(host='xn--2h3bo3h03e', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001AE9C2857D0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "ERROR:root:HTTPS 연결 오류 - 임종성: HTTPSConnectionPool(host='xn--oj4br9ibwa', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001AE9C2857D0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "ERROR:root:HTTP 연결 오류 - 임종성: HTTPConnectionPool(host='xn--oj4br9ibwa', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001AE9C285750>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "ERROR:root:HTTPS 연결 오류 - 010-5770-3387: HTTPSConnectionPool(host='010-5770-3387', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001AE9C285050>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "ERROR:root:HTTP 연결 오류 - 010-5770-3387: HTTPConnectionPool(host='010-5770-3387', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001AE9C284050>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url x 스미싱 위험도 낮음\n"
     ]
    }
   ],
   "source": [
    "# url이 없는 메세지\n",
    "\n",
    "text1 = \"\"\"카드배송업체 마그넷서비스\n",
    "[롯데카드] 정*은님이 신청하신 카드가 오늘 11:00~13:00 배송될 예정 입니다. \n",
    "■ 배송지 주소: 서울 성북구 장위로51길 15 \n",
    "■ 배송업체: 마그넷서비스\n",
    "■ 배송원: 임종성 010-5770-3387\"\"\"\n",
    "\n",
    "print(\"url이 없는 메세지:\\n\")\n",
    "print(url_keywords_extraction(text1, keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a35ab24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:HTTPS 연결 오류 - [카카오뱅크]: Failed to parse: https://[카카오뱅크]\n",
      "ERROR:root:HTTP 연결 오류 - [카카오뱅크]: Failed to parse: http://[카카오뱅크]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정상 url이 있는 메세지:\n",
      "\n",
      "db 스미싱 위험도 낮음\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Db에 있는 정상 url\n",
    "\n",
    "text2 = \"\"\"대출 금리변동 안내\n",
    "[Web발신]\n",
    "[카카오뱅크] \n",
    "청년 전월세보증금 대출(4197)의 금리가 2023.12.13에 4.903%로 변경되었습니다. \n",
    "\n",
    "[금리 변경 내역]\n",
    "- 변경 전: 4.373%(신규COFIX6개월 3.440% + 0.933%)\n",
    "- 변경 후: 4.903%(신규COFIX6개월 3.970% + 0.933%)\n",
    "\n",
    "자세한 사항은 https://www.kakaobank.com 에서 확인해주세요\"\"\"\n",
    "\n",
    "print(\"정상 url이 있는 메세지:\\n\")\n",
    "print(url_keywords_extraction(text2,keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49ac92dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Db에 없지만 정상 url:\n",
      "\n",
      "ml 스미싱 위험도 낮음\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Db에 없지만 정상 url이 있는 메세지\n",
    "\n",
    "text3 = \"lamenteesmaravillosa.com\"\n",
    "\n",
    "print(\"Db에 없지만 정상 url이 있는 메세지:\\n\")\n",
    "print(url_keywords_extraction(text3,keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4317e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Db에 없고 정상 아닌 거:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:HTTPS 연결 오류 - https://is.gd/MUxrJ7?odlrzi: HTTPSConnectionPool(host='https', port=443): Max retries exceeded with url: //is.gd/MUxrJ7?odlrzi (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001AE9C3BFDD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "ERROR:root:HTTP 연결 오류 - https://is.gd/MUxrJ7?odlrzi: HTTPConnectionPool(host='https', port=80): Max retries exceeded with url: //is.gd/MUxrJ7?odlrzi (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001AE99829A90>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error creating BeautifulSoup object: HTTPConnectionPool(host='qt-es.top', port=80): Max retries exceeded with url: /?95484 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001AE9C28D110>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Error trying to connect to socket: closing socket - [Errno 11001] getaddrinfo failed\n",
      "ml 스미싱 위험도 높음\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 피싱 url이 있는 메세지 (학습에 쓰이지 않은 url)\n",
    "\n",
    "text4 = \"https://is.gd/MUxrJ7?odlrzi\"\n",
    "\n",
    "print(\"피싱 url이 있는 메세지:\\n\")\n",
    "print(url_keywords_extraction(text4, keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd939bbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
